{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Clinton's mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import the pandas library to handle tables of data in a convenient way. Other Python modules are used and will be loaded when needed:\n",
    "* re\n",
    "* collections\n",
    "* networkx (pip install networkx)\n",
    "* matplotlib (pip install matplotlib)\n",
    "* community (pip install python-louvain, for the community detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the file. We only need the text of the emails, so only ```Emails.csv``` is necessary. You should get it from the Kaggle repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Emails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: filter the text, search for proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual in data analysis, the preprocessing steps are crucial and take a large part of the analysis code and of the analysis time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find some important words in the texts, and get rid of the useless articles for example. We could use a Natural Language Processing toolbox, there are several in Python. However, I want to keep this example simple. So we will select the proper nouns in the texts which can be found easily because the begin with a capital letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "capitalized_word_list =[]\n",
    "filtered_text_list = []\n",
    "for row in df.itertuples():\n",
    "    # The text are filtered with regex, keeping only alphanumeric characters\n",
    "    filtered_text = re.findall('\\w+', str(row.ExtractedBodyText), re.UNICODE)\n",
    "    filtered_text_list.append(filtered_text)\n",
    "    capitalized_in_single_text= [word for word in filtered_text if word.istitle()]\n",
    "    [capitalized_word_list.append(Word) for Word in capitalized_in_single_text]\n",
    "# For each email, we keep only the email Id, the text and the date \n",
    "dataframe_f = df[['Id','ExtractedBodyText','MetadataDateSent']].copy()\n",
    "# We add the filtered text\n",
    "dataframe_f.loc[:,'filtered_text'] = filtered_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a list of Capitalized words appearing in the emails ```Word_list``` and a table ```dataframe_f``` containing the text and some info about the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all words beginning with a Capital letter are proper nouns. The first word of each sentence has a capital as well. In the next step we get rid of the words that appear frequently both with or without a capital letter. It is sign that they are not proper nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the capitalized words and keep only the proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first turn the list of capitalized words into a dataframe  of words and their respective occurence in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Word_dic = Counter(capitalized_word_list)\n",
    "Word_df = pd.DataFrame(list(Word_dic.items()),columns=['word','occur'])\n",
    "sorted_words = Word_df.sort_values('occur',ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the number of words by keeping only the ones appearing more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of capitalized words 10074\n",
      "Number of capitalized words appearing more than 20 times: 811\n"
     ]
    }
   ],
   "source": [
    "print('Number of capitalized words',len(sorted_words))\n",
    "threshold = 20\n",
    "sorted_words = sorted_words[sorted_words.occur > threshold]\n",
    "print('Number of capitalized words appearing more than {} times: {}'.format(threshold,len(sorted_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each capitalized word, let us record the number of times it appears with no capital letter in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 0 ns, total: 25.9 s\n",
      "Wall time: 25.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lowc_word_dic = {}\n",
    "for word in sorted_words.word:\n",
    "    word_lc = word.lower()\n",
    "    count = 0\n",
    "    # for each text, search the word in lower case and count the nb of times it appears\n",
    "    for row in dataframe_f.itertuples():\n",
    "        wordlist = row.filtered_text\n",
    "        if len(set(wordlist)&set([word_lc]))>0:\n",
    "            word_indices = [i for i, x in enumerate(wordlist) if x == word_lc]\n",
    "            count += len(word_indices)\n",
    "    lowc_word_dic[word] = count\n",
    "# Create a new dataframe with the words and their occurence in lower case\n",
    "Wordlc_df = pd.DataFrame(list(lowc_word_dic.items()),columns=['word','lc_occur'])\n",
    "# Merge the data from capitalized / lower case into a single dataframe\n",
    "df_1 = pd.merge(Word_df, Wordlc_df, on='word', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the proper noun that appear more often in capital than in lower case *and* do not appear in lower case more than 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a capital: 10074\n",
      "Number of words appearing more in capital:: 544\n",
      "Number of words appearing less than 100 times without capital: 530\n"
     ]
    }
   ],
   "source": [
    "print('Number of words with a capital: {}'.format(len(df_1)))\n",
    "df_2 = df_1[df_1.occur>df_1.lc_occur].sort_values('occur',ascending=False)\n",
    "print('Number of words appearing more in capital:: {}'.format(len(df_2)))\n",
    "df_3 = df_2[df_2.lc_occur<100].sort_values('occur',ascending=False)\n",
    "print('Number of words appearing less than 100 times without capital: {}'.format(len(df_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the graph are the word obtained in the previous processing. \n",
    "\n",
    "Two words are linked by an edge if they appear together in at least one email. The weight of the edge is the number of time the nodes appear together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "G = nx.Graph()\n",
    "for wordlist in dataframe_f.filtered_text:\n",
    "    wordset = set(wordlist)&set(df_3.word.tolist())\n",
    "    if len(wordset)>0:\n",
    "        couples = itertools.combinations(wordset, 2)\n",
    "        for edge in couples:\n",
    "            if G.has_edge(edge[0],edge[1]):\n",
    "                # just increase the weight by one\n",
    "                G[edge[0]][edge[1]]['weight'] += 1\n",
    "            else:\n",
    "                # new edge with weight=1\n",
    "                G.add_edge(edge[0], edge[1], weight=1)\n",
    "        #G.add_edges_from(couples)\n",
    "        #[G.add_edge(couple) for couple in couples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute some node and edge properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean weight: 3.69035016287, max weight: 239\n"
     ]
    }
   ],
   "source": [
    "# Rescaling edge weights\n",
    "n1,n2,weights = zip(*G.edges(data='weight'))\n",
    "edges_id=list(zip(n1,n2))\n",
    "import numpy as np\n",
    "print('mean weight: '+str(np.mean(weights))+', max weight: '+str(np.max(weights)))\n",
    "weights_n=weights/np.max(weights)\n",
    "weights_n_dic=dict(zip(edges_id,weights_n)) \n",
    "nx.set_edge_attributes(G,'weight_norm',weights_n_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Node properties: degree and centrality\n",
    "degreeDic = G.degree(weight='weight_norm')\n",
    "nx.set_node_attributes(G,'degree',degreeDic)\n",
    "#bcDic = nx.betweenness_centrality(G)\n",
    "#nx.set_node_attributes(G,'bCentrality',bcDic)\n",
    "degreeDic = G.degree()\n",
    "nx.set_node_attributes(G,'degree_n',degreeDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(G):\n",
    "    # Normalize the weights of a graph\n",
    "    # store the values in the property 'weight_n'\n",
    "    degreeDic = G.degree(weight='weight')\n",
    "    nx.set_node_attributes(G,'degree',degreeDic)\n",
    "    for node1,node2,data in G.edges(data=True):\n",
    "        degree1 = np.sqrt(G.node[node1]['degree'])\n",
    "        degree2 = np.sqrt(G.node[node2]['degree'])\n",
    "        weight = G[node1][node2]['weight']\n",
    "        G[node1][node2]['weight_n'] = weight/degree1/degree2\n",
    "    #nx.set_node_attributes(G,'degree',1)\n",
    "    degreeDic = G.degree(weight='weight')\n",
    "    nx.set_node_attributes(G,'degree',degreeDic)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = normalize_weights(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "#first compute the best partition\n",
    "clusterDic = community.best_partition(G)\n",
    "nx.set_node_attributes(G,'cluster',clusterDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-14517dce2195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "nx.draw_networkx(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size: 1997\n",
      "Final size: 1385\n"
     ]
    }
   ],
   "source": [
    "# Remove the weakest edges of the most connected nodes\n",
    "# uses the normalized weights\n",
    "print('Initial size: {}'.format(G.size()))\n",
    "for u,v,a in G.edges(data=True):\n",
    "    if a['weight_n']<0.009:\n",
    "        G.remove_edge(u,v)\n",
    "print('Final size: {}'.format(G.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size: 68768\n",
      "Final size: 1233\n"
     ]
    }
   ],
   "source": [
    "# Remove some edges\n",
    "print('Initial size: {}'.format(G.size()))\n",
    "for u,v,a in G.edges(data=True):\n",
    "    if a['weight']<20:\n",
    "        G.remove_edge(u,v)\n",
    "print('Final size: {}'.format(G.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the graph to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the graph to a json file\n",
    "from networkx.readwrite import json_graph\n",
    "datag = json_graph.node_link_data(G)\n",
    "import json\n",
    "s = json.dumps(datag)\n",
    "datag['links'] = [\n",
    "        {\n",
    "            'source': datag['nodes'][link['source']]['id'],\n",
    "            'target': datag['nodes'][link['target']]['id'],\n",
    "            'weight': link['weight']\n",
    "        }\n",
    "        for link in datag['links']]\n",
    "s = json.dumps(datag)\n",
    "with open(\"viz/HCgraph.json\", \"w\") as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge nodes with high edge weight (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_nodes(G,node1,node2,data=False):\n",
    "    \"\"\" merge two nodes node1 and node2 in the graph into one node \n",
    "        with id node1+'_'+node2\n",
    "        if data=None, any data is ignored\n",
    "        if data='node1' or 'node2' the new node inherit the data of the given node and\n",
    "        the degree of all nodes is recomputed after merging\n",
    "    \"\"\"\n",
    "    H = G.copy()\n",
    "    # create the new node\n",
    "    node_id = node1+'_'+node2\n",
    "    if data==False:\n",
    "        H.add_node(node_id)\n",
    "    elif data==True:\n",
    "        degree1 = len(G[node1])\n",
    "        degree2 = len(G[node2])\n",
    "        if degree1>degree2:\n",
    "            H.add_node(node_id,H.node[node1])\n",
    "        else:\n",
    "            H.add_node(node_id,H.node[node2])\n",
    "    else:\n",
    "        raise ValueError(\"data only accept True or False\")\n",
    "    # connect it to the rest\n",
    "    for n,n_data in H[node1].items():\n",
    "        if not (n==node2 or n==node1):\n",
    "            H.add_edge(node_id,n,weight = n_data['weight'])\n",
    "    for n,n_data in H[node2].items():\n",
    "        if not (n==node1 or n==node2):\n",
    "            H.add_edge(node_id,n,weight = n_data['weight'])\n",
    "    # remove the initial nodes and edges\n",
    "    H.remove_node(node1)\n",
    "    H.remove_node(node2)\n",
    "    # compute new nodes properties\n",
    "    # TODO: recompute only for the neighbors of the merged nodes\n",
    "    H = normalize_weights(H)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean weight: 0.00312859414067, max weight: 0.241071428571\n"
     ]
    }
   ],
   "source": [
    "n1,n2,weights = zip(*G.edges(data='weight_n'))\n",
    "import numpy as np\n",
    "print('mean weight: '+str(np.mean(weights))+', max weight: '+str(np.max(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cingular Xpress\n",
      "Cingular_Xpress Blackberry\n",
      "Valmoro Lona\n",
      "Jiloty Lauren\n",
      "Abedin Huma\n",
      "Lanka Sri\n",
      "Cheryl Mills\n",
      "Nora Joanne\n",
      "Assistant Valmoro_Lona\n",
      "B1 Declassify\n",
      "Talbott Strobe\n",
      "En Residence\n",
      "Waldorf Astoria\n",
      "Sullivan Jacob\n",
      "Sullivan_Jacob J\n",
      "Cherie Blair\n",
      "Verveer Melanne\n",
      "Marie Anne\n",
      "En_Residence Room\n",
      "Date Doc\n",
      "H Fw\n",
      "B6 B5\n",
      "Thx Ok\n",
      "Pak Af\n",
      "B1_Declassify B\n",
      "Slaughter Marie_Anne\n",
      "Floor En_Residence_Room\n",
      "Camera Treaty\n",
      "Craig Arturo\n",
      "Thank Nora_Joanne\n",
      "Date_Doc F\n",
      "Megan C05739767\n",
      "Outer Floor_En_Residence_Room\n",
      "Camera_Treaty Outer_Floor_En_Residence_Room\n",
      "Feltman Jeffrey\n",
      "Airport Andrews\n",
      "Talbott_Strobe Brookings\n",
      "Camera_Treaty_Outer_Floor_En_Residence_Room Department\n",
      "Reg Empey\n",
      "Jiloty_Lauren C\n",
      "Abedin_Huma Assistant_Valmoro_Lona\n",
      "Cc Oscar\n",
      "Fein Sinn\n",
      "Tel Verveer_Melanne\n",
      "Robinson Shaun\n",
      "Ulster Unionist\n",
      "States United\n",
      "Cherie_Blair Tony\n",
      "Mexican Juarez\n",
      "Cheryl_Mills Sullivan_Jacob_J\n"
     ]
    }
   ],
   "source": [
    "# merge the edges with a high normalized weight\n",
    "for i in range(50):\n",
    "    source,target,data=max(G.edges(data=True), key=(lambda data: data[2]['weight_n']))\n",
    "    if data['weight_n']>0.02:\n",
    "        print(source,target)\n",
    "        G = merge_nodes(G,source,target,data=True)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in G.edges(data=True):\n",
    "    if edge[2]['weight_n']>0.25:\n",
    "        print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
