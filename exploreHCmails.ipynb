{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Clinton's emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import the pandas library to handle tables of data in a convenient way. Other Python modules are used and will be loaded when needed:\n",
    "* re, collections, json\n",
    "* networkx (pip install networkx)\n",
    "* community (pip install python-louvain, for the community detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the file. We only need the text of the emails, so only ```Emails.csv``` is necessary. You should get it from the Kaggle repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7945"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: filter the text, search for proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual in data analysis, the preprocessing steps are crucial and take a large part of the analysis code and of the analysis time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find some important words in the texts, and get rid of the useless articles for example. We could use a Natural Language Processing toolbox, there are several in Python. However, I want to keep this example simple. So we will select the proper nouns in the texts which can be found easily because they begin with a capital letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "capitalized_word_list =[]\n",
    "filtered_text_list = []\n",
    "for row in df.itertuples():\n",
    "    # The text are filtered with regex, keeping only alphanumeric characters\n",
    "    filtered_text = re.findall('\\w+', str(row.ExtractedBodyText), re.UNICODE)\n",
    "    filtered_text_list.append(filtered_text)\n",
    "    capitalized_in_single_text= [word for word in filtered_text if word.istitle()]\n",
    "    [capitalized_word_list.append(Word) for Word in capitalized_in_single_text]\n",
    "# For each email, we keep only the email Id, the text and the date \n",
    "dataframe_f = df[['Id','ExtractedBodyText','MetadataDateSent']].copy()\n",
    "# We add the filtered text\n",
    "dataframe_f.loc[:,'filtered_text'] = filtered_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a list of Capitalized words appearing in the emails ```Word_list``` and a table ```dataframe_f``` containing the text and some info about the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all words beginning with a Capital letter are proper nouns. The first word of each sentence has a capital as well. In the next step we get rid of the words that appear frequently both with or without a capital letter. It is sign that they are not proper nouns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the capitalized words and keep only the proper nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first turn the list of capitalized words into a dataframe  of words and their respective occurence in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Word_dic = Counter(capitalized_word_list)\n",
    "Word_df = pd.DataFrame(list(Word_dic.items()),columns=['word','occur'])\n",
    "sorted_words = Word_df.sort_values('occur',ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the number of words by keeping only the ones appearing more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of capitalized words 10074\n",
      "Number of capitalized words appearing more than 20 times: 811\n"
     ]
    }
   ],
   "source": [
    "print('Number of capitalized words',len(sorted_words))\n",
    "threshold = 20\n",
    "sorted_words = sorted_words[sorted_words.occur > threshold]\n",
    "print('Number of capitalized words appearing more than {} times: {}'.format(threshold,len(sorted_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each capitalized word, let us record the number of times it appears with no capital letter in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.9 s, sys: 40 ms, total: 26 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lowc_word_dic = {}\n",
    "for word in sorted_words.word:\n",
    "    word_lc = word.lower()\n",
    "    count = 0\n",
    "    # for each text, search the word in lower case and count the nb of times it appears\n",
    "    for row in dataframe_f.itertuples():\n",
    "        wordlist = row.filtered_text\n",
    "        if len(set(wordlist)&set([word_lc]))>0:\n",
    "            word_indices = [i for i, x in enumerate(wordlist) if x == word_lc]\n",
    "            count += len(word_indices)\n",
    "    lowc_word_dic[word] = count\n",
    "# Create a new dataframe with the words and their occurence in lower case\n",
    "Wordlc_df = pd.DataFrame(list(lowc_word_dic.items()),columns=['word','lc_occur'])\n",
    "# Merge the data from capitalized / lower case into a single dataframe\n",
    "df_1 = pd.merge(Word_df, Wordlc_df, on='word', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the proper noun that appear more often in capital than in lower case *and* do not appear in lower case more than 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with a capital: 10074\n",
      "Number of words appearing more in capital: 544\n",
      "Number of words appearing less than 100 times without capital: 530\n"
     ]
    }
   ],
   "source": [
    "print('Number of words with a capital: {}'.format(len(df_1)))\n",
    "df_2 = df_1[df_1.occur>df_1.lc_occur].sort_values('occur',ascending=False)\n",
    "print('Number of words appearing more in capital: {}'.format(len(df_2)))\n",
    "df_3 = df_2[df_2.lc_occur<100].sort_values('occur',ascending=False)\n",
    "print('Number of words appearing less than 100 times without capital: {}'.format(len(df_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes of the graph are the word obtained in the previous processing. \n",
    "\n",
    "Two words are linked by an edge if they appear together in at least one email. The weight of the edge is the number of time the nodes appear together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "G = nx.Graph()\n",
    "for wordlist in dataframe_f.filtered_text:\n",
    "    wordset = set(wordlist)&set(df_3.word.tolist())\n",
    "    if len(wordset)>0:\n",
    "        couples = itertools.combinations(wordset, 2)\n",
    "        for edge in couples:\n",
    "            if G.has_edge(edge[0],edge[1]):\n",
    "                # just increase the weight by one\n",
    "                G[edge[0]][edge[1]]['weight'] += 1\n",
    "            else:\n",
    "                # new edge with weight=1\n",
    "                G.add_edge(edge[0], edge[1], weight=1)\n",
    "        #G.add_edges_from(couples)\n",
    "        #[G.add_edge(couple) for couple in couples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a nicer and more informative visualization we can run a community detection algorithm on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "#first compute the best partition\n",
    "clusterDic = community.best_partition(G)\n",
    "nx.set_node_attributes(G,'cluster',clusterDic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of edges: 68768\n",
      "mean edge weight: 3.69035016287, max edge weight: 239\n"
     ]
    }
   ],
   "source": [
    "# Edge info\n",
    "print('Nb of edges: {}'.format(G.size()))\n",
    "n1,n2,weights = zip(*G.edges(data='weight'))\n",
    "import numpy as np\n",
    "print('mean edge weight: '+str(np.mean(weights))+', max edge weight: '+str(np.max(weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is too connected to be visualized like that. We are going to reduce the number of connections. But we do not want to simply delete the weakest connections, otherwise many disconnected nodes could be created. Instead we will remove the edges with low weight, compared to the (geometric) mean degree of the nodes it connects. In other words, we will keep the strongest connections of each node. This way, the number of overly connected nodes (hubs) will be reduced and the nodes with a few weak connections will keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial nb of edges: 68768\n",
      "Final nb of edges: 1715\n"
     ]
    }
   ],
   "source": [
    "# Remove some edges\n",
    "print('Initial nb of edges: {}'.format(G.size()))\n",
    "for u,v,a in G.edges(data=True):\n",
    "    mean_node_degree = np.sqrt(G.degree(u, weight='weight')*G.degree(v, weight='weight'))\n",
    "    if a['weight']<0.035*mean_node_degree:\n",
    "        G.remove_edge(u,v)\n",
    "print('Final nb of edges: {}'.format(G.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the graph to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the degree of each node, used in the visualization (node size)\n",
    "degreeDic = G.degree(weight='weight')\n",
    "nx.set_node_attributes(G,'degree',degreeDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the graph to a json file\n",
    "from networkx.readwrite import json_graph\n",
    "datag = json_graph.node_link_data(G)\n",
    "import json\n",
    "s = json.dumps(datag)\n",
    "datag['links'] = [\n",
    "        {\n",
    "            'source': datag['nodes'][link['source']]['id'],\n",
    "            'target': datag['nodes'][link['target']]['id'],\n",
    "            'weight': link['weight']\n",
    "        }\n",
    "        for link in datag['links']]\n",
    "s = json.dumps(datag)\n",
    "with open(\"docs/HCgraph2.json\", \"w\") as f:\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
